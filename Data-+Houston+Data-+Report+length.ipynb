{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Project: Wrangle OSM Data to SQL- Houston, Texas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I chose my hometown of Houston, Texas. I originally chose a small area containing Rice University and the museum district then moved my way to the larger metropolitan area. As the data covers a large area, I was unable to efficiently run my code on the full dataset and instead chose to sample the area using code provided by Udacity. I used the code provided and created during the Problem Set exercises as the basis for collecting and auditing the data. From there, I created the csv files and database (which proved to be one of the most difficult parts). I then used the sample project as a template for formulating questions for querying using sqlite. In an effort to keep this report brief, I have trimmed much of the auditing code, results, and sql queries to be kept separately. \n",
    "\n",
    "OSM query: \n",
    "https://www.openstreetmap.org/relation/2688911\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing/Improving Street Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial audit: \n",
    "I used code developed during the case study to audit the street names (using regular expressions) for problematic characters, valid lower case characters, and other characters. Luckily enough there were 0 problematic characters in my dataset. \n",
    "\n",
    "Improvements: \n",
    "Most of the below code and comments were taken from the problem set prior to this project. I ran the code repeatedly to better understand the types of street names that would be caught and adjusted accordingly. Some streets had programatic fixes while others just needed a very specific one-off fix. While a bit tedious, this approach worked for this data set but may not have worked for a larger one. \n",
    "\n",
    "If needed, the code could be rewritten to search through each word in a street name rather than just the street type (which I believe was a suggestion on the Udacity forums). I thought it sufficient to look for incorrect/mispelled street types rather than the various ways one could write a street type. For my purposes, I saw no reason to go into the details of modifying and standardizing the highway names and types, especially with data that is put together by users and would likely have similar discrepancies in other cities' datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The below code has been shortened for this report\n",
    "\"\"\"\n",
    "- audit the OSMFILE and use the variable 'mapping' which reflects the changes needed \n",
    "    Note: a semi-generalized solution \n",
    "- update_name: actually fixes the street name.\n",
    "    The function takes a string with street name as an argument and should return the fixed name\n",
    "\"\"\"\n",
    "\n",
    "OSMFILE = \"interpreter.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Circle\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "           \"1960\", \"6\", \"Real\", \"Highway\", \"Trace\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\", \n",
    "            \"Stree\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",  \n",
    "            }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    street_type = street_type_re.search(name).group()\n",
    "    if street_type not in expected: \n",
    "        if street_type in mapping:\n",
    "            name = (name[:-len(street_type)] + mapping[street_type])\n",
    "            #print name \n",
    "        else: \n",
    "            #print \"name not in mapping:\", street_type            \n",
    "            unmapped.add(name)\n",
    "            #just in case we want to take a look and make sure nothing crazy is in the unmapped set        \n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schema provided by Udacity was created and loaded. Then data was gathered using iterparse, shaped appropriately using the schema, and written to CSVs. Lastly, the SQLite3 database was created along with relevant tables, and the data loaded using DictReader. From here, it was much easier to explore the data using SQLite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:  66749940\n",
      "Nodes CSV:  23374752\n",
      "Nodes Tags CSV:  564723\n",
      "Ways CSV:  2452684\n",
      "Ways Tags CSV:  6043017\n",
      "Ways Nodes CSV:  8296465\n",
      "Database:  35467264\n"
     ]
    }
   ],
   "source": [
    "# File size \n",
    "import os\n",
    "print \"Original Data: \", os.path.getsize('/Users/irasema/Desktop/DataScience/Udacity/Data Analyst/Project 2/interpreter.osm')\n",
    "print \"Nodes CSV: \", os.path.getsize('/Users/irasema/Desktop/DataScience/Udacity/Data Analyst/Project 2/nodes.csv')\n",
    "print \"Nodes Tags CSV: \", os.path.getsize('/Users/irasema/Desktop/DataScience/Udacity/Data Analyst/Project 2/nodes_tags.csv')\n",
    "print \"Ways CSV: \", os.path.getsize('/Users/irasema/Desktop/DataScience/Udacity/Data Analyst/Project 2/ways.csv')\n",
    "print \"Ways Tags CSV: \", os.path.getsize('/Users/irasema/Desktop/DataScience/Udacity/Data Analyst/Project 2/ways_tags.csv')\n",
    "print \"Ways Nodes CSV: \", os.path.getsize('/Users/irasema/Desktop/DataScience/Udacity/Data Analyst/Project 2/ways_nodes.csv')\n",
    "print \"Database: \", os.path.getsize('/Users/irasema/Desktop/DataScience/Udacity/Data Analyst/Project 2/interpreter.db')\n",
    "\n",
    "#Resource:\n",
    "#https://stackoverflow.com/questions/6591931/getting-file-size-in-python\n",
    "#https://docs.python.org/3/library/os.path.html#os.path.getsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are all data coordinates within the limits of the original query? Is anything out of place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0          1         2          3\n",
      "0  30.1831  29.463502 -94.82301 -96.100149\n"
     ]
    }
   ],
   "source": [
    "query = ''' select max(lat), min(lat), max(lon), min(lon) from nodes '''\n",
    "db = sqlite3.connect(\"interpreter.db\")\n",
    "cursor = db.cursor()\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates seem to all fall within the Houston metropolitan area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many Nodes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0\n",
      "0  278542\n"
     ]
    }
   ],
   "source": [
    "cursor = db.cursor()\n",
    "query = \"select count(*) from nodes\"\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\n",
      "0  41283\n"
     ]
    }
   ],
   "source": [
    "query = \"Select count(*) from ways\"\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the most common node tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0                  1     2\n",
      "0     highway     turning_circle  2313\n",
      "1       power              tower  1379\n",
      "2       power               pole   932\n",
      "3     highway    traffic_signals   751\n",
      "4     highway           crossing   358\n",
      "5     highway       turning_loop   331\n",
      "6     railway     level_crossing   321\n",
      "7    state_id                 48   273\n",
      "8   county_id                201   225\n",
      "9       state                 TX   219\n",
      "10    natural               tree   205\n",
      "11    amenity   place_of_worship   193\n",
      "12   religion          christian   188\n",
      "13       city            Houston   177\n",
      "14    barrier               gate   163\n",
      "15     noexit                yes   135\n",
      "16   building              house   127\n",
      "17    highway  motorway_junction   113\n",
      "18    created         12/08/2003   112\n",
      "19   crossing              zebra    91\n"
     ]
    }
   ],
   "source": [
    "query = ''' select key, value, count(*) as count from nodetags \n",
    "group by key, value order by count desc limit 20 \n",
    "'''\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are there so many zebra crossing tags?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon further exploration, (looking at key/value pairs, googling the lat/lon, and googling the phrase 'zebra crossings osm') it seems the zebra crossing is a type of crosswalk (nonspecific to zebras or painted as zebras for the Houston Zoo). \n",
    "\n",
    "This may be an example of not when understanding the data leads to findings that may seem off but after some digging, actually make sense. I figured though the result was not noteable and could wholely be taken out of the report, the investigative procedure is helpful to demonstrate. \n",
    "\n",
    "https://wiki.openstreetmap.org/wiki/Approved_features/Road_crossings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0        1\n",
      "0        brianboru     9065\n",
      "1        davidearl     3582\n",
      "2  woodpeck_repair   145231\n",
      "3        andrewpmk     1679\n",
      "4  woodpeck_fixbot   147510\n",
      "5          scottyc   496606\n",
      "6         afdreher  1110270\n",
      "7           clay_c   119881\n",
      "8    RoadGeek_MD99   475877\n",
      "9          Memoire  2176227\n"
     ]
    }
   ],
   "source": [
    "# This query combines the nodes and ways table to find all users who have contributed to either\n",
    "# Limiting to 10 for readability\n",
    "query = ''' select distinct(subq.user), uid\n",
    "from (select uid, user from nodes union all select uid, user from ways) subq\n",
    "limit 10\n",
    "'''\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 contributors? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0                1      2\n",
      "0  1110270         afdreher  50264\n",
      "1   147510  woodpeck_fixbot  35051\n",
      "2   496606          scottyc  19328\n",
      "3  3119079          cammace  19259\n",
      "4   119881           clay_c  16125\n",
      "5     9065        brianboru  11224\n",
      "6   243003          skquinn   8680\n",
      "7   475877    RoadGeek_MD99   7622\n",
      "8   672878         TexasNHD   7005\n",
      "9  2176227          Memoire   6540\n"
     ]
    }
   ],
   "source": [
    "# top 10 contributing users across the nodes and ways tables \n",
    "query = ''' select uid, subq.user, count(*) as count\n",
    "from \n",
    "(select uid, user from nodes union all select uid, user from ways) as subq\n",
    "group by subq.user\n",
    "order by count desc\n",
    "limit 10\n",
    "'''\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users with 1 post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "0  265\n"
     ]
    }
   ],
   "source": [
    "query = ''' \n",
    "select count (*) from \n",
    "(select uid, user, count(*) as counts\n",
    "from (select uid, user from nodes union all select uid, user from ways) as subq\n",
    "group by subq.user\n",
    "having counts = 1\n",
    "order by counts desc ) as substuff \n",
    "'''\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0         1   2\n",
      "0             Subway  sandwich  11\n",
      "1            Wendy's    burger   7\n",
      "2    Jack in the Box    burger   4\n",
      "3         McDonald's    burger   4\n",
      "4        Chick-fil-A   chicken   3\n",
      "5  Schlotzsky's Deli  sandwich   3\n",
      "6        Whataburger    burger   3\n",
      "7        Burger King    burger   2\n",
      "8        Jamba Juice    drinks   2\n",
      "9                KFC   chicken   2\n"
     ]
    }
   ],
   "source": [
    "# What are the most popular places and their corresponding cuisine? \n",
    "# Note, this query only draws from nodetags (not waytags - to be addressed later) \n",
    "# Limit 10 for readability \n",
    "query = ''' select names.name, value, count (*) as count \n",
    "from nodetags, \n",
    "(SELECT distinct(id) as restid FROM nodetags WHERE value= 'restaurant' or value= 'fast_food') \n",
    "as rest, \n",
    "(select value as name, id as nameid from nodetags where key = 'name') as names \n",
    "\n",
    "where nodetags.id = rest.restid \n",
    "and nodetags.id = names.nameid \n",
    "and nodetags.key = 'cuisine' \n",
    "\n",
    "group by name \n",
    "order by count desc \n",
    "limit 10 \n",
    "'''\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common amenities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0    1\n",
      "0   place_of_worship  193\n",
      "1          fast_food   89\n",
      "2           fountain   89\n",
      "3         restaurant   81\n",
      "4             school   57\n",
      "5              bench   33\n",
      "6       fire_station   24\n",
      "7               fuel   23\n",
      "8               bank   20\n",
      "9           pharmacy   20\n",
      "10              cafe   19\n",
      "11           toilets   12\n",
      "12            police   11\n",
      "13    drinking_water    9\n",
      "14               atm    8\n"
     ]
    }
   ],
   "source": [
    "# Note this query only accounts for node tags \n",
    "query = ''' select value, count(*) as count from nodetags \n",
    "where key = 'amenity' \n",
    "group by value\n",
    "order by count desc\n",
    "limit 15\n",
    "'''\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Ideas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we've observed that node tags and way tags tend to be similar and hold similar pieces of information. It seems that some information may be split across the two tables such that if you were only querying from one, your result may be incomplete and results thus misleading. \n",
    "\n",
    "For example, searching for restaurants in an area by querying nodetags may lead you to overlook a choice if additional, unique restaurants were held in the waytags table instead. You may end up missing out on your favorite restaurant. \n",
    "\n",
    "To remedy this, you may try to fix the problem at data input, or combine both tables for your queries. \n",
    "\n",
    "At data input, additional instructions on creating data for organization can be provided to those contributing to OSM's data. Cycling through to rearrange data from one table to another would be long, tedious, and difficult. Depending on the query, combining tags from both ways and nodes should be sufficient to gather relevant data in the same place. \n",
    "\n",
    "Additionally, for further data analysis, it would be more beneficial to read through the possible values for tags first, which should be provided on the OSM wiki. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a view that combines way and node tags \n",
    "query = ''' create view alltags as select * from nodetags union all select * from waytags '''\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0         1   2\n",
      "0        Whataburger    burger  16\n",
      "1             Subway  sandwich  12\n",
      "2         McDonald's    burger   8\n",
      "3            Wendy's    burger   8\n",
      "4        Chick-fil-A   chicken   6\n",
      "5    Jack in the Box    burger   6\n",
      "6        Burger King    burger   5\n",
      "7      Panda Express   chinese   4\n",
      "8  Schlotzsky's Deli  sandwich   4\n",
      "9              Sonic    burger   3\n"
     ]
    }
   ],
   "source": [
    "# What are the most prevalent restaurants in the area? \n",
    "# Note, this query draws from alltags \n",
    "query = ''' select names.name, value, count (*) as count \n",
    "from \n",
    "alltags, \n",
    "(SELECT distinct(id) as restid FROM alltags WHERE value= 'restaurant' or value= 'fast_food') \n",
    "as rest, \n",
    "(select value as name, id as nameid from alltags where key = 'name') as names \n",
    "\n",
    "where alltags.id = rest.restid \n",
    "and alltags.id = names.nameid \n",
    "and alltags.key = 'cuisine' \n",
    "\n",
    "group by name \n",
    "order by count desc \n",
    "limit 10\n",
    "'''\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "df = pd.DataFrame(rows)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon assessment, it seems that for data collected by humans, it is fairly consistent and thorough (at least more than expected). While this project was challenging, the OSM data seems like a valuable resource for compiling data and practicing data manipulation. Most of my frustrations actually occured with preparing the database and creating the database from the csv files. \n",
    "\n",
    "If our purpose was to clean the OSM data, then I would likely approach this differently in the future. I thought it better to correct mistakes rather than create a standardization with manmade (ish) data. Instead of standardizing something in one subset of data which may be different in another (depending on the auditor's preference i.e. highway vs hwy), it may be better to compile the various ways contributors may add a data point (Road vs Rd vs Rd.) and group them under the same category when analyzing the data\n",
    "\n",
    "More familiarization is likely needed with the OSM standards in terms of data possibilities (tags) and organization for other projects. However, many of the top contributors can likely share their code with contributors in other areas in order to speed up the process of preparing and cleaning. \n",
    "\n",
    "It seems the challenges faced with this dataset had numerous ways (ha) of being approached, and it would be interesting to see how the top contributors structure their bots to clean the data.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
